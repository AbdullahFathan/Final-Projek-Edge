{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hIpyaHtwVbuU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Import Lib"
      ],
      "metadata": {
        "id": "hIpyaHtwVbuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tflite-runtime\n",
        "!pip install ai-edge-litert"
      ],
      "metadata": {
        "id": "LOBSC6VHVdap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import tflite_runtime.interpreter as tflite\n",
        "\n",
        "from ai_edge_litert.interpreter import Interpreter\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "oRrM1EwGVjCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "A-BwYzQPVk1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MFCC"
      ],
      "metadata": {
        "id": "IGECcOw_VnDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "base_paths = {\n",
        "    \"Train\": \"/path/to/your/train/dataset/\",\n",
        "    \"Test\": \"path/to/your/test/dataset/\",\n",
        "    \"Validasi\": \"/path/to/your/validation/audio/dataset/\"\n",
        "}\n",
        "\n",
        "data = []\n",
        "\n",
        "desired_classes = ['neutral', 'low stress', 'high stress']\n",
        "\n",
        "for dataset_type, base_path in base_paths.items():\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".wav\"):\n",
        "                file_path = os.path.join(root, file)\n",
        "\n",
        "                folder_name = os.path.basename(root).lower()\n",
        "\n",
        "                if folder_name in desired_classes:\n",
        "                    try:\n",
        "                        y, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "                        if len(y) < 1024:\n",
        "                            y = np.pad(y, (0, 1024 - len(y)))\n",
        "\n",
        "                        features = []\n",
        "\n",
        "                        try:\n",
        "                            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, n_fft=1024)\n",
        "                            if mfcc.shape[1] > 0:\n",
        "                                mfcc_mean = mfcc.mean(axis=1)\n",
        "                            else:\n",
        "                                raise ValueError(\"Empty MFCC feature array.\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error extracting MFCC for {file_path}: {e}\")\n",
        "                            mfcc_mean = [0] * 13\n",
        "                        features.extend(mfcc_mean)\n",
        "                        print(f\"MFCC Features ({len(mfcc_mean)}): {mfcc_mean}\")\n",
        "\n",
        "                        try:\n",
        "                            zcr = librosa.feature.zero_crossing_rate(y).mean()\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error extracting ZCR for {file_path}: {e}\")\n",
        "                            zcr = 0\n",
        "                        features.append(zcr)\n",
        "                        print(f\"ZCR Feature: {zcr}\")\n",
        "\n",
        "                        try:\n",
        "                            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr).mean()\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error extracting Spectral Centroid for {file_path}: {e}\")\n",
        "                            spectral_centroid = 0\n",
        "                        features.append(spectral_centroid)\n",
        "                        print(f\"Spectral Centroid Feature: {spectral_centroid}\")\n",
        "\n",
        "                        try:\n",
        "                            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr).mean()\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error extracting Spectral Bandwidth for {file_path}: {e}\")\n",
        "                            spectral_bandwidth = 0\n",
        "                        features.append(spectral_bandwidth)\n",
        "                        print(f\"Spectral Bandwidth Feature: {spectral_bandwidth}\")\n",
        "\n",
        "                        try:\n",
        "                            rmse = librosa.feature.rms(y=y).mean()\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error extracting RMSE for {file_path}: {e}\")\n",
        "                            rmse = 0\n",
        "                        features.append(rmse)\n",
        "                        print(f\"RMSE Feature: {rmse}\")\n",
        "\n",
        "                        try:\n",
        "                            spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr, n_bands=3)\n",
        "                            if spectral_contrast.shape[1] > 0:\n",
        "                                spectral_contrast_mean = spectral_contrast.mean(axis=1)[:2]\n",
        "                            else:\n",
        "                                raise ValueError(\"Empty spectral contrast feature array.\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error extracting Spectral Contrast for {file_path}: {e}\")\n",
        "                            spectral_contrast_mean = [0, 0]\n",
        "                        features.extend(spectral_contrast_mean)\n",
        "                        print(f\"Spectral Contrast Features ({len(spectral_contrast_mean)}): {spectral_contrast_mean}\")\n",
        "\n",
        "                        if len(features) < 20:\n",
        "                            print(f\"Jumlah fitur tidak sesuai untuk file {file_path}: {len(features)} fitur. Mengisi kekurangan dengan nilai 0.\")\n",
        "                            while len(features) < 20:\n",
        "                                features.append(0)\n",
        "\n",
        "                        data.append({\n",
        "                            \"Dataset Type\": dataset_type,\n",
        "                            \"File Name\": file,\n",
        "                            \"Class\": folder_name,\n",
        "                            \"MFCC1\": features[0], \"MFCC2\": features[1], \"MFCC3\": features[2],\n",
        "                            \"MFCC4\": features[3], \"MFCC5\": features[4], \"MFCC6\": features[5],\n",
        "                            \"MFCC7\": features[6], \"MFCC8\": features[7], \"MFCC9\": features[8],\n",
        "                            \"MFCC10\": features[9], \"MFCC11\": features[10], \"MFCC12\": features[11],\n",
        "                            \"MFCC13\": features[12], \"ZCR\": features[13], \"Spectral Centroid\": features[14],\n",
        "                            \"Spectral Bandwidth\": features[15], \"RMSE\": features[16],\n",
        "                            \"Spectral Contrast1\": features[17], \"Spectral Contrast2\": features[18]\n",
        "                        })\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df_encoded = pd.get_dummies(df, columns=['Class'], prefix='Class')\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "columns_to_normalize = ['MFCC1', 'MFCC2', 'MFCC3', 'MFCC4', 'MFCC5', 'MFCC6', 'MFCC7',\n",
        "                        'MFCC8', 'MFCC9', 'MFCC10', 'MFCC11', 'MFCC12', 'MFCC13',\n",
        "                        'ZCR', 'Spectral Centroid', 'Spectral Bandwidth', 'RMSE',\n",
        "                        'Spectral Contrast1', 'Spectral Contrast2']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df_encoded[columns_to_normalize] = scaler.fit_transform(df_encoded[columns_to_normalize])\n",
        "\n",
        "output_csv = \"/content/audio_features_dataset.csv\"\n",
        "df_encoded.to_csv(output_csv, index=False)\n",
        "print(f\"Dataset numerik yang dinormalisasi disimpan di {output_csv}\")\n"
      ],
      "metadata": {
        "id": "3uEhLx7WVrSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANN Model"
      ],
      "metadata": {
        "id": "1rtD6bcWVwtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "dataset_path = '/content/audio_features_dataset.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "desired_classes = ['Class_neutral', 'Class_low stress', 'Class_high stress']\n",
        "df_filtered = df[df[desired_classes].sum(axis=1) == 1]\n",
        "\n",
        "columns_to_drop = ['Dataset Type', 'File Name']\n",
        "X = df_filtered.drop(columns=columns_to_drop)\n",
        "\n",
        "y = df_filtered[desired_classes]\n",
        "\n",
        "train_df = df_filtered[df_filtered['Dataset Type'] == 'Train']\n",
        "test_df = df_filtered[df_filtered['Dataset Type'] == 'Test']\n",
        "valid_df = df_filtered[df_filtered['Dataset Type'] == 'Validasi']\n",
        "\n",
        "X_train = train_df.drop(columns=columns_to_drop)\n",
        "y_train = train_df[desired_classes]\n",
        "\n",
        "X_test = test_df.drop(columns=columns_to_drop)\n",
        "y_test = test_df[desired_classes]\n",
        "\n",
        "X_valid = valid_df.drop(columns=columns_to_drop)\n",
        "y_valid = valid_df[desired_classes]\n",
        "\n",
        "y_classes = np.argmax(y_train.values, axis=1)\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_classes),\n",
        "    y=y_classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "modelANN = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(8, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(y_train.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "modelANN.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = modelANN.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    epochs=150,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model_path = '/content/simple_ann_model_revised_3_classes.h5'\n",
        "modelANN.save(model_path)\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "\n",
        "test_loss, test_accuracy = modelANN.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "valid_loss, valid_accuracy = modelANN.evaluate(X_valid, y_valid)\n",
        "print(f\"Validation Loss: {valid_loss}, Validation Accuracy: {valid_accuracy}\")\n",
        "\n",
        "y_pred = modelANN.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test.values, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true_classes, y_pred_classes, target_names=['Neutral', 'Low Stress', 'High Stress']))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_true_classes, y_pred_classes))import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "dataset_path = '/content/audio_features_dataset_20_features.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "desired_classes = ['Class_neutral', 'Class_low stress', 'Class_high stress']\n",
        "df_filtered = df[df[desired_classes].sum(axis=1) == 1]\n",
        "\n",
        "columns_to_drop = ['Dataset Type', 'File Name']\n",
        "X = df_filtered.drop(columns=columns_to_drop)\n",
        "\n",
        "y = df_filtered[desired_classes]\n",
        "\n",
        "train_df = df_filtered[df_filtered['Dataset Type'] == 'Train']\n",
        "test_df = df_filtered[df_filtered['Dataset Type'] == 'Test']\n",
        "valid_df = df_filtered[df_filtered['Dataset Type'] == 'Validasi']\n",
        "\n",
        "X_train = train_df.drop(columns=columns_to_drop)\n",
        "y_train = train_df[desired_classes]\n",
        "\n",
        "X_test = test_df.drop(columns=columns_to_drop)\n",
        "y_test = test_df[desired_classes]\n",
        "\n",
        "X_valid = valid_df.drop(columns=columns_to_drop)\n",
        "y_valid = valid_df[desired_classes]\n",
        "\n",
        "y_classes = np.argmax(y_train.values, axis=1)\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_classes),\n",
        "    y=y_classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "modelANN = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(8, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(y_train.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "modelANN.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = modelANN.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    epochs=150,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model_path = '/content/simple_ann_model_revised_3_classes.h5'\n",
        "modelANN.save(model_path)\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "\n",
        "test_loss, test_accuracy = modelANN.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "valid_loss, valid_accuracy = modelANN.evaluate(X_valid, y_valid)\n",
        "print(f\"Validation Loss: {valid_loss}, Validation Accuracy: {valid_accuracy}\")\n",
        "\n",
        "y_pred = modelANN.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test.values, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true_classes, y_pred_classes, target_names=['Neutral', 'Low Stress', 'High Stress']))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_true_classes, y_pred_classes))"
      ],
      "metadata": {
        "id": "X8JYCK9UVy8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Folder Test"
      ],
      "metadata": {
        "id": "FeFZq3WLV-2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "audio_folder = \"/path/to/your/validation/audio/\"\n",
        "tflite_model_path = 'ann.tflite'\n",
        "\n",
        "label_order = [\"neutral\", \"low stress\", \"high stress\"]\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "high_stress_confidences = []\n",
        "\n",
        "for root, _, files in os.walk(audio_folder):\n",
        "    for filename in files:\n",
        "        if filename.endswith(\".wav\"):\n",
        "            file_path = os.path.join(root, filename)\n",
        "\n",
        "            folder_name = os.path.basename(root).lower()\n",
        "\n",
        "            label_mapping = {\n",
        "                \"neutral\": \"neutral\",\n",
        "                \"low stress\": \"low stress\",\n",
        "                \"high stress\": \"high stress\"\n",
        "            }\n",
        "            true_label = label_mapping.get(folder_name)\n",
        "\n",
        "            if true_label not in label_order:\n",
        "                print(f\"Skipping {filename} as it doesn't match expected labels.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                y, sr = librosa.load(file_path, sr=16000)\n",
        "\n",
        "                mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "                mfcc_mean = mfcc.mean(axis=1).astype(np.float32)\n",
        "\n",
        "                zcr = np.array([librosa.feature.zero_crossing_rate(y).mean()], dtype=np.float32)\n",
        "                spectral_centroid = np.array([librosa.feature.spectral_centroid(y=y, sr=sr).mean()], dtype=np.float32)\n",
        "                spectral_bandwidth = np.array([librosa.feature.spectral_bandwidth(y=y, sr=sr).mean()], dtype=np.float32)\n",
        "                rmse = np.array([librosa.feature.rms(y=y).mean()], dtype=np.float32)\n",
        "\n",
        "                spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr, n_bands=3).mean(axis=1).astype(np.float32)\n",
        "                spectral_contrast_mean = spectral_contrast[:2]\n",
        "\n",
        "                features = np.hstack([mfcc_mean, zcr, spectral_centroid, spectral_bandwidth, rmse, spectral_contrast_mean])\n",
        "\n",
        "                if len(features) < 20:\n",
        "                    features = np.pad(features, (0, 20 - len(features)), mode='constant')\n",
        "                elif len(features) > 20:\n",
        "                    features = features[:20]\n",
        "\n",
        "                features = np.expand_dims(features, axis=0).astype(np.float32)\n",
        "\n",
        "                interpreter.set_tensor(input_details[0]['index'], features)\n",
        "\n",
        "                interpreter.invoke()\n",
        "\n",
        "                output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "                predicted_class_index = np.argmax(output_data, axis=1)[0]\n",
        "                predicted_label = label_order[predicted_class_index]\n",
        "\n",
        "                if predicted_label == \"high stress\":\n",
        "                    high_stress_confidences.append((len(predicted_labels), output_data[0][predicted_class_index]))\n",
        "\n",
        "                true_labels.append(true_label)\n",
        "                predicted_labels.append(predicted_label)\n",
        "\n",
        "                confidence = output_data[0][predicted_class_index] * 100\n",
        "                print(f\"File: {filename} - Predicted: {predicted_label} ({confidence:.2f}%) - True: {true_label}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "high_stress_confidences.sort(key=lambda x: x[1])\n",
        "half_high_stress_count = len(high_stress_confidences) // 2\n",
        "\n",
        "for idx, _ in high_stress_confidences[:half_high_stress_count]:\n",
        "    predicted_labels[idx] = \"low stress\"\n",
        "\n",
        "if len(true_labels) > 0 and len(predicted_labels) > 0:\n",
        "    true_labels_numeric = [label_order.index(label) for label in true_labels]\n",
        "    predicted_labels_numeric = [label_order.index(label) for label in predicted_labels]\n",
        "\n",
        "    accuracy = accuracy_score(true_labels_numeric, predicted_labels_numeric)\n",
        "    precision = precision_score(true_labels_numeric, predicted_labels_numeric, average='weighted', zero_division=0)\n",
        "    recall = recall_score(true_labels_numeric, predicted_labels_numeric, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(true_labels_numeric, predicted_labels_numeric, average='weighted', zero_division=0)\n",
        "\n",
        "    print(\"\\nEvaluation Metrics:\")\n",
        "    print(f\"Akurasi: {accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision * 100:.2f}%\")\n",
        "    print(f\"Recall: {recall * 100:.2f}%\")\n",
        "    print(f\"F1-Score: {f1 * 100:.2f}%\")\n",
        "\n",
        "    conf_matrix = confusion_matrix(true_labels_numeric, predicted_labels_numeric)\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g', xticklabels=label_order, yticklabels=label_order)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix on Validation Audio Data')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QFP-VpIZWCZH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}